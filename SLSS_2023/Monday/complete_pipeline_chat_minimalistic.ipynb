{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586248df",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation\n",
    " \n",
    "In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. \n",
    "\n",
    "This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334357c",
   "metadata": {},
   "source": [
    "![](overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d106d9c42f42f",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5343d8b3358f3",
   "metadata": {},
   "source": [
    "![](data_loading_sources.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a249fa3b63fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key  = \"paste-your-api-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load all PDF\n",
    "loaders = [PyPDFLoader(f\"docs/{file}\") for file in os.listdir('docs') if file.endswith('.pdf')]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    pdf_docs = loader.load()\n",
    "    docs += pdf_docs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6fdc8caf20bea2"
  },
  {
   "cell_type": "markdown",
   "id": "7e2765b5b03ef194",
   "metadata": {},
   "source": [
    "# Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3a247",
   "metadata": {},
   "source": [
    "![](document_splitting.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492cba024fd3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844da49f8c7b66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150,\n",
    "    separators=[\"\\n\\n\", \"(?<=\\. )\", \"\\n\", \" \", \"\"],\n",
    "    is_separator_regex = True\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "for doc in splits:\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace('- ', \"\")\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d11494aa84d74b",
   "metadata": {},
   "source": [
    "# Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1d8e8",
   "metadata": {},
   "source": [
    "![](embeddings.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2eb0bfd4ad59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you have paid version of OpenAI \n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# embedding = OpenAIEmbeddings(openai_api_key=\"paste-your-api-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this if you don't have paid version of OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a2af773b8cfdd",
   "metadata": {},
   "source": [
    "### Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8140915d11627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943a7b038654be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e2fc11c6ee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493906cf6bf57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425ddfd7599af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e05ccce559bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534a859bded80d18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](overview.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f21fe12144d34eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question Answering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a0acb77f46f4ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm_name = \"gpt-3.5-turbo\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42be154480e7c8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key=\"paste-your-api-key-here\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd66c56c219ed62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RetrievalQA chain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c737055105f70fc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4079516db8700e54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5, \"distance_metric\": \"cos\"})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48183abb92c02d82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Tell me something interesting about IMBH.\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e4017f78f28503e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"How do IMBH compare to super-massive black holes?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35109f81d3b3e6c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result['source_documents']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83647e1ba6ba1030"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Which type of black holes are the most common?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f0d2cbc217669ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"What is a typical density of IMBH?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c91bc65a764ae644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"What is a typical size of IMBH?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb162fc4664b1f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Do we have some IMBH in our galaxy?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e90db5e33f370"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4828b586625d3797"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Default prompts:\n",
    "https://github.com/samrawal/langchain-prompts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f407eec915a99c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea1133713d224c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 8, \"distance_metric\": \"cos\"}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cf717543c16783e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"How do IMBH compare to super-massive black holes?\"})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172d437601ba083"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat\n",
    "\n",
    "Recall the overall workflow for retrieval augmented generation (RAG):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5febddeddfb5438a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![overview.jpeg](overview.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f1b31288a566222"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc4137c6a1314f35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55071b1c36f77879"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ConversationalRetrievalChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5abd5bc5eaf6350a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e031cdd1ef8844bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Tell me something interesting about IMBH.\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24ab5654c1ba8320"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Which of these theories is the most popular among scientists?\"\n",
    "result = qa({\"question\": question})\n",
    "result['answer']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca07e01f4bb85365"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
