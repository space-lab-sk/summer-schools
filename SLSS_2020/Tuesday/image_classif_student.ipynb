{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib tensorflow keras opencv-python graphviz scikit-learn scikit-image sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.feature import peak_local_max, corner_peaks\n",
    "import sqlite3\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier, plot_feature_importances, plot_decision_tree\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional programming vs. Machine Learning (ML)\n",
    "<img src=\"imgs/tradit_vs_ML.png\" width=\"640\" height=\"320\">\n",
    "\n",
    "### AI / ML / DL\n",
    "<img src=\"imgs/AI_ML_DL.jpg\" width=\"640\" height=\"320\">\n",
    "\n",
    "### Features vs. Labels / Targets\n",
    "<img src=\"imgs/feat_labels.jpg\" width=\"500\" height=\"250\">\n",
    "\n",
    "### ML vs. DL\n",
    "<img src=\"imgs/ML_DL.jpg\" width=\"600\" height=\"300\">\n",
    "\n",
    "### Supervised vs. Unsupervised learning\n",
    "<img src=\"imgs/super_unsuper.jpg\" width=\"600\" height=\"300\">\n",
    "\n",
    "### Classification vs. Regression\n",
    "<img src=\"imgs/class_regr.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'AAC_dataset/usable/'\n",
    "files = [file for file in os.listdir(path) if file.endswith('.npy')]\n",
    "image = np.load(path + files[0])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_height = 300 \n",
    "new_width = 300\n",
    "data_path = 'AAC_dataset/'\n",
    "class_mappings = {}\n",
    "\n",
    "subfolders = [folder for folder in os.listdir(data_path) if not folder.startswith('.')]\n",
    "X, y, timestamps = [], [], []\n",
    "for i, subfolder in enumerate(subfolders):\n",
    "    class_mappings[i] = subfolder\n",
    "    files = [file for file in os.listdir(data_path + subfolder) if file.endswith('.npy')]\n",
    "    for file in files:\n",
    "        image = np.load(data_path + subfolder + '/' + file)\n",
    "        image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        X.append(image)\n",
    "        y.append(i)\n",
    "        parsed_ts = pd.to_datetime(file.split('.')[0][4:], format='%Y-%m-%d_%H-%M-%S')\n",
    "        timestamps.append(parsed_ts)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "WLFUpFcjr5UI",
    "outputId": "503f1a9b-44f6-48d9-fa53-7c8ac24cd800",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_ids, counts = np.unique(y, return_counts=True)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar([class_mappings[class_id] for class_id in class_ids], counts)\n",
    "plt.ylabel('Number of images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize classes - raw images\n",
    "fig, axs = plt.subplots(figsize=(9.5, 6), nrows=3, ncols=4)\n",
    "\n",
    "for class_id, class_name in class_mappings.items():\n",
    "    class_subset = np.where(y == class_id)[0]\n",
    "    rnd_idx = np.random.choice(class_subset)\n",
    "    \n",
    "    for j in range(4):\n",
    "        im = axs[class_id, j].imshow(X[rnd_idx, :, :, j], cmap='gray')\n",
    "        fig.colorbar(im, ax=axs[class_id, j])\n",
    "        axs[class_id, j].set_title(class_name + '_' + str(rnd_idx) + '_' + str(j))\n",
    "        axs[class_id, j].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize classes - log images\n",
    "fig, axs = plt.subplots(figsize=(9.5, 6), nrows=3, ncols=4)\n",
    "\n",
    "for class_id, class_name in class_mappings.items():\n",
    "    class_subset = np.where(y == class_id)[0]\n",
    "    rnd_idx = np.random.choice(class_subset)\n",
    "    \n",
    "    for j in range(4):\n",
    "        im = axs[class_id, j].imshow(np.log(1 + X[rnd_idx, :, :, j]), cmap='gray')\n",
    "        fig.colorbar(im, ax=axs[class_id, j])\n",
    "        axs[class_id, j].set_title(class_name + '_' + str(rnd_idx) + '_' + str(j))\n",
    "        axs[class_id, j].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize classes - quantile images\n",
    "fig, axs = plt.subplots(figsize=(9.5, 6), nrows=3, ncols=4)\n",
    "\n",
    "for class_id, class_name in class_mappings.items():\n",
    "    class_subset = np.where(y == class_id)[0]\n",
    "    rnd_idx = np.random.choice(class_subset)\n",
    "    \n",
    "    for j in range(4):\n",
    "        X_slice = X[rnd_idx, :, :, j]\n",
    "        im = axs[class_id, j].imshow(X_slice, \n",
    "                                     cmap='gray', \n",
    "                                     vmin=np.percentile(X_slice, 50), \n",
    "                                     vmax=np.percentile(X_slice, 99))\n",
    "        fig.colorbar(im, ax=axs[class_id, j])\n",
    "        axs[class_id, j].set_title(class_name + '_' + str(rnd_idx) + '_' + str(j))\n",
    "        axs[class_id, j].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame()\n",
    "feats['timestamp'] = timestamps\n",
    "feats['max_all'] = [img_set.max() for img_set in X]\n",
    "feats['mean_all'] = [img_set.mean() for img_set in X]\n",
    "for i in range(4):\n",
    "    feats['max_' + str(i)] = [img_set[:, :, i].max() for img_set in X]\n",
    "    feats['mean_' + str(i)] = [img_set[:, :, i].mean() for img_set in X]\n",
    "\n",
    "y_onehot = to_categorical(y)\n",
    "feats['y_unusable'] = y_onehot[:, 0].astype(int)\n",
    "feats['y_spoilt'] = y_onehot[:, 1].astype(int)\n",
    "feats['y_usable'] = y_onehot[:, 2].astype(int)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transf = []\n",
    "for img_set in X:\n",
    "    X_transf_1 = img_set.astype(float)\n",
    "    for j in range(4):\n",
    "        X_slice = X_transf_1[:, :, j]\n",
    "        p1 = np.percentile(X_slice, 50)\n",
    "        p2 = np.percentile(X_slice, 99)\n",
    "        X_slice[X_slice <= p1] = p1\n",
    "        X_slice[X_slice >= p2] = p2\n",
    "        X_slice -= X_slice.min()\n",
    "        X_slice /= X_slice.max()\n",
    "    X_transf.append(X_transf_1)\n",
    "X_transf = np.array(X_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = X_transf[130, :, :, 3]\n",
    "# coordinates = peak_local_max(test_img, min_distance=10, threshold_abs=0.3)\n",
    "coordinates = corner_peaks(test_img, min_distance=5, threshold_abs=0.3)\n",
    "print(len(coordinates))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.plot(coordinates[:, 1], coordinates[:, 0], 'r.', markersize=2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_peaks = []\n",
    "for img_set in X_transf:\n",
    "    peak_sum = 0\n",
    "    for j in range(4):\n",
    "        peak_sum += len(corner_peaks(img_set[:, :, j], min_distance=5, threshold_abs=0.3))\n",
    "    nb_peaks.append(peak_sum)\n",
    "\n",
    "feats['nb_peaks'] = nb_peaks\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_final = feats[['timestamp', 'max_all', 'mean_all', 'nb_peaks', 'black_pixs']].copy()\n",
    "feats_final['y'] = y\n",
    "feats_final = feats_final.sort_values(by='timestamp')\n",
    "feats_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train = feats_final.iloc[:180, 1:5].copy()\n",
    "X_test = feats_final.iloc[180:, 1:5].copy()\n",
    "y_train = feats_final.iloc[:180, 5].copy()\n",
    "y_test = feats_final.iloc[180:, 5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "for col in X_train.columns:    \n",
    "    intercept = X_train[col].min()\n",
    "    scale = (X_train[col] - intercept).max()\n",
    "    X_train[col] = (X_train[col] - intercept) / scale\n",
    "    X_test[col] = (X_test[col] - intercept) / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=10)\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_subset = [2, 3]\n",
    "feature_names = list(X_train.columns[feat_subset])\n",
    "X_subset_tr = X_train.values[:, feat_subset].copy()\n",
    "X_subset_te = X_test.values[:, feat_subset].copy()\n",
    "\n",
    "lr = LogisticRegression(C=10)\n",
    "lr.fit(X_subset_tr, y_train)\n",
    "print(lr.score(X_subset_tr, y_train))\n",
    "print(lr.score(X_subset_te, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_class_regions_for_classifier(lr, \n",
    "                                  X_subset_tr, \n",
    "                                  y_train.values, \n",
    "                                  X_test=X_subset_te, \n",
    "                                  y_test=y_test.values, \n",
    "                                  title='Logistic regression',\n",
    "                                  target_names=list(class_mappings.values()),\n",
    "                                  feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_train, y_train))\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print('======== ' + str(i) + ' ========')\n",
    "    print(knn.score(X_train, y_train))\n",
    "    print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_subset = [0, 3]\n",
    "feature_names = list(X_train.columns[feat_subset])\n",
    "X_subset_tr = X_train.values[:, feat_subset].copy()\n",
    "X_subset_te = X_test.values[:, feat_subset].copy()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_subset_tr, y_train)\n",
    "print(knn.score(X_subset_tr, y_train))\n",
    "print(knn.score(X_subset_te, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_regions_for_classifier(knn, \n",
    "                                  X_subset_tr, \n",
    "                                  y_train.values, \n",
    "                                  X_test=X_subset_te, \n",
    "                                  y_test=y_test.values, \n",
    "                                  title='K nearest neighbours',\n",
    "                                  target_names=list(class_mappings.values()),\n",
    "                                  feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt4 = DecisionTreeClassifier(max_depth=5)\n",
    "dt4.fit(X_train, y_train)\n",
    "print(dt4.score(X_train, y_train))\n",
    "print(dt4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(dt4, X_train.columns, list(class_mappings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(dt4, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_subset = [0, 3]\n",
    "feature_names = list(X_train.columns[feat_subset])\n",
    "X_subset_tr = X_train.values[:, feat_subset].copy()\n",
    "X_subset_te = X_test.values[:, feat_subset].copy()\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(X_subset_tr, y_train)\n",
    "print(dt.score(X_subset_tr, y_train))\n",
    "print(dt.score(X_subset_te, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_regions_for_classifier(dt, \n",
    "                                  X_subset_tr, \n",
    "                                  y_train.values, \n",
    "                                  X_test=X_subset_te, \n",
    "                                  y_test=y_test.values, \n",
    "                                  title='K nearest neighbours',\n",
    "                                  target_names=list(class_mappings.values()),\n",
    "                                  feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=4, n_estimators=200)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.score(X_train, y_train))\n",
    "print(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with datetime train/test separation (more fair)\n",
    "# separate images for train/test, normalize images\n",
    "\n",
    "break_date = feats_final.iloc[180, 0]\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = [], [], [], []\n",
    "for i in range(len(timestamps)):\n",
    "    if timestamps[i] < break_date:\n",
    "        X_train2.append(X_transf[i])\n",
    "        y_train2.append(y_onehot[i])\n",
    "    else:\n",
    "        X_test2.append(X_transf[i])\n",
    "        y_test2.append(y_onehot[i])\n",
    "        \n",
    "X_train2 = np.array(X_train2)\n",
    "X_test2 = np.array(X_test2)\n",
    "y_train2 = np.array(y_train2)\n",
    "y_test2 = np.array(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X_train2[100, :, :, 2], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_arch(drop_rate=0):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(300, 300, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = create_model_arch(drop_rate=0.0)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "               optimizer=tf.keras.optimizers.Adam(lr),\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                                   rotation_range=45)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "train_generator = train_datagen.flow(\n",
    "        X_train2,\n",
    "        y=y_train2,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "validation_generator = test_datagen.flow(\n",
    "        X_test2,\n",
    "        y=y_test2,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcp_save = ModelCheckpoint('im_quality_pred.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "history = model3.fit(train_generator,\n",
    "                     steps_per_epoch=len(X_train2) // batch_size,\n",
    "                     epochs=50,\n",
    "                     validation_data=validation_generator,\n",
    "                     validation_steps=len(X_test2) // batch_size,\n",
    "                     callbacks=[mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9.5, 4))\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[0].set_title('Model loss')\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "    axs[1].plot(history.history['accuracy'])\n",
    "    axs[1].plot(history.history['val_accuracy'])\n",
    "    axs[1].set_title('Model accuracy')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"ae_test.db\")\n",
    "aac = pd.read_sql('select * from aac', con)\n",
    "aac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aac['timestamp'] = pd.to_datetime(aac['timestamp'])\n",
    "X_norm = pd.concat([X_train, X_test])\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt4.predict(X_norm)\n",
    "X_norm['y_pred'] = y_pred\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = pd.merge(X_norm, feats['timestamp'], left_index=True, right_index=True, how='inner')\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(X_norm[['timestamp', 'y_pred']], aac, left_on='timestamp', right_on='timestamp', how='right')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=['class'])\n",
    "final_df = final_df.rename(columns={'y_pred': 'class'})[['timestamp', 'image_name', 'class']]\n",
    "final_df['timestamp'] = final_df['timestamp'].astype(str)\n",
    "final_df.to_sql('aac2', con, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "esca_competition.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
