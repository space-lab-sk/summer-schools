{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e93052-dd49-4be1-ac9b-77a23a2ee54a",
   "metadata": {},
   "source": [
    "# üåû Summer School 2024: Advanced Data Science and Time Series Analysis\n",
    "\n",
    "Welcome to Summer School 2024! This session is designed to take you on an in-depth journey through advanced data science concepts, with a special focus on time series analysis, model evaluation, and visualization using Python. Whether you're a beginner or an experienced practitioner, this notebook will guide you through practical applications in data preprocessing, feature selection, and model performance evaluation, with a specific emphasis on working with real-world datasets like space weather data.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Load, clean, and preprocess data using `pandas`\n",
    "- Perform correlation analysis and visualize relationships between variables\n",
    "- Prepare and manipulate time series data for autoregressive modeling\n",
    "- Train and evaluate a Bidirectional LSTM model for time series prediction\n",
    "- Assess model performance using both regression and classification metrics\n",
    "- Visualize results to draw meaningful insights from data\n",
    "\n",
    "## üõ†Ô∏è Tools and Libraries\n",
    "\n",
    "Throughout this notebook, we will utilize the following tools and libraries to facilitate our data science workflow:\n",
    "- **Pandas**: For data manipulation, loading, and preprocessing\n",
    "- **Seaborn**: For creating intuitive and visually appealing statistical plots\n",
    "- **Matplotlib**: The backbone for all our visualizations and plots\n",
    "- **Scikit-learn**: For feature selection and model evaluation metrics\n",
    "- **TensorFlow/Keras**: For building, training, and evaluating our LSTM models\n",
    "\n",
    "## üìä Dataset Overview\n",
    "\n",
    "The dataset we'll be using is sourced from the `omni_full.csv` file, which contains a range of space weather-related attributes. This data is vital for understanding the effects of solar activities on Earth's magnetosphere, and in this session, we will focus on predicting the Disturbance Storm Time (DST) index‚Äîa key measure of geomagnetic activity.\n",
    "\n",
    "## üîç What You Will Do\n",
    "\n",
    "1. **Data Loading and Exploration**:\n",
    "   - Start by loading the dataset and exploring its structure to gain a clear understanding of the attributes available for analysis.\n",
    "\n",
    "2. **Data Cleaning and Preprocessing**:\n",
    "   - Clean the dataset by removing unnecessary columns and handling missing values. This step ensures that the data is ready for analysis and modeling.\n",
    "\n",
    "3. **Correlation Analysis and Visualization**:\n",
    "   - Perform a detailed correlation analysis to identify relationships between different attributes, and visualize these relationships using heatmaps.\n",
    "\n",
    "4. **Feature Selection and Data Splitting**:\n",
    "   - Select the most relevant features for predicting the DST index using statistical methods, and split the dataset into training, validation, and test sets to ensure robust model evaluation.\n",
    "\n",
    "5. **Time Series Data Preparation**:\n",
    "   - Prepare the time series data for an autoregressive LSTM model using `TimeseriesGenerator`. Understand how to manipulate sequential data for time series forecasting.\n",
    "\n",
    "6. **Model Training and Evaluation**:\n",
    "   - Train a Bidirectional LSTM model to predict future values of the DST index. Evaluate the model's performance using key metrics such as Mean Squared Error (MSE), Pearson Correlation Coefficient (PCC), and Matthews Correlation Coefficient (MCC).\n",
    "\n",
    "7. **Final Model Evaluation**:\n",
    "   - Apply both regression and classification metrics to assess the final model's performance on unseen test data, ensuring that it generalizes well to new observations.\n",
    "\n",
    "## üìà Let's Get Started!\n",
    "\n",
    "Follow the instructions in each section of this notebook, and feel free to experiment with different parameters and approaches. This is a hands-on learning experience, so dive in and explore the fascinating world of data science and time series analysis. If you have any questions or need assistance, don't hesitate to ask. Let's make this session both educational and enjoyable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6775236-478f-4fce-8e92-2cb37967387a",
   "metadata": {},
   "source": [
    "## üì¶ Importing Necessary Libraries\n",
    "\n",
    "We start by importing the necessary libraries and modules that will be used throughout this notebook. \n",
    "#These include libraries for data manipulation, visualization, feature selection, and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afe242-946e-4ce1-8876-d999366d9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn modules for feature selection and evaluation metrics\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# TensorFlow/Keras modules for model creation, training, and evaluation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, TimeDistributed, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Additional Scikit-learn metrics for classification tasks\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26844f1-4460-42d7-92eb-4d20fac26313",
   "metadata": {},
   "source": [
    "## üåê Running This Notebook on Google Colab\n",
    "\n",
    "To ensure that everyone can follow along and execute the notebook without any local setup issues, we recommend using Google Colab. Google Colab is a free, cloud-based platform that allows you to run Jupyter notebooks with access to powerful computing resources, including GPUs.\n",
    "\n",
    "### Steps to Open and Run the Notebook on Google Colab:\n",
    "\n",
    "1. **Open the Notebook**:\n",
    "   - Click on the following link to open the notebook directly in Google Colab: [Open in Google Colab]().\n",
    "\n",
    "2. **Clone the Repository**:\n",
    "   - Once the notebook is open in Colab, you'll need to clone the entire repository to access all necessary files and data.\n",
    "   - Run the following command in a new code cell to clone the repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085c367-d297-4fd4-b8f0-298732348902",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VieraMaslej/summer_school_2024.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db37ec3-e4ef-48e7-b34e-3dacd9446ae2",
   "metadata": {},
   "source": [
    "3. **Install Git LFS (Large File Storage):**\n",
    "\n",
    "    - Git LFS is used to handle large files in the repository. If you encounter any issues with file sizes or missing files, it's likely because Git LFS hasn't been installed or initialized.\n",
    "    - Run the following commands to install Git LFS and initialize it:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f76ecd-7928-4969-a96f-977f72446a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91c30f-ff7f-44c0-b86b-fac5e009d36b",
   "metadata": {},
   "source": [
    "\n",
    "4. **Pull the LFS Files:**\n",
    "\n",
    "    - Navigate to the `data` directory within the cloned repository and pull the large files tracked by Git LFS:\n",
    "    \n",
    "*Comment*: The `%cd` command is used to change the working directory to `data` within the repository. Then, `git lfs pull` ensures that any large files tracked by LFS are downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32801525-3434-4cec-afd2-5d37323a9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd summer_school_2024/data\n",
    "!git lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3dfc01-64ab-45a3-a24e-753f294b7675",
   "metadata": {},
   "source": [
    "\n",
    "5. **Create Necessary Directories:**\n",
    "\n",
    "    - Create the `results` and `models` directories if they do not already exist. These directories will store your output files and trained models:\n",
    "    \n",
    "*Comment*: The `-p` flag ensures that the directories are only created if they do not already exist, preventing errors.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49449db0-7327-4ad7-90a6-75807c770d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../results\n",
    "!mkdir -p ../models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbd5a7-79a6-4d14-afca-bbd1832065e7",
   "metadata": {},
   "source": [
    "\n",
    "6. **Navigate Back to the Notebook Directory:**\n",
    "\n",
    "    - Return to the main directory or `notebook` directory to continue running the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937db61f-a591-4c31-aa24-38e853302a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487db56-bbf2-4141-b1ef-e2cbc2536586",
   "metadata": {},
   "source": [
    "## üìÇ Loading the Dataset\n",
    "\n",
    "Here, we load our dataset from the CSV file `omni_full.csv`. This dataset contains various space weather-related attributes that we will analyze and visualize in the following sections.\n",
    "\n",
    "The OMNI dataset is available online. The dataset was obtained using the Aidapy and Heliopy libraries.\n",
    "\n",
    "Source of the dataset: [https://spdf.gsfc.nasa.gov/pub/data/omni/low_res_omni/](https://spdf.gsfc.nasa.gov/pub/data/omni/low_res_omni/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083830bf-94ab-4ed4-9665-f8ea6290fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/omni_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68e244-13ad-4c85-b522-f1aece0caefd",
   "metadata": {},
   "source": [
    "## üîç Exploring the Dataset\n",
    "\n",
    "To better understand the structure of our dataset, we start by examining the columns it contains. This will give us an overview of the attributes available for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e72920-436b-4aba-baab-360a6551a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd468e-55af-459a-9a63-fb0025070bae",
   "metadata": {},
   "source": [
    "## üßπ Cleaning the Dataset\n",
    "\n",
    "In this step, we clean the dataset by removing unnecessary columns that won't be needed for our analysis. This helps to simplify the dataset and focus on the most relevant attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c8ab0-3396-4c91-aabc-74b63ee2c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Unnamed: 0.1', 'Unnamed: 0', 'BZ_GSM-1', 'BZ_GSM-2', 'BZ_GSM-3', 'BZ_GSM-4', 'BZ_GSM-5', 'BZ_GSM-6',\n",
    "       'BZ_GSM-7', 'BZ_GSM-8', 'BZ_GSM-9', 'BZ_GSM-10', 'BZ_GSM-11',\n",
    "       'BZ_GSM-12', 'BZ_GSM-13', 'BZ_GSM-14', 'BZ_GSM-15', 'BZ_GSM-16',\n",
    "       'BZ_GSM-17', 'BZ_GSM-18', 'BZ_GSM-19', 'BZ_GSM-20', 'BZ_GSM-21',\n",
    "       'BZ_GSM-22', 'BZ_GSM-23', 'BZ_GSM-24'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680976f-0dae-46f2-9311-64a713d4e07d",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Checking Dataset Dimensions\n",
    "\n",
    "After cleaning the dataset, it's important to check the shape of the data. This will tell us the number of rows and columns that remain, giving us a sense of the dataset's size and scope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b76d23-7176-495b-8f0a-69342ed88707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394ad16-9f5c-42c8-85ff-c0553237819f",
   "metadata": {},
   "source": [
    "## üìä Descriptive Statistics for DST\n",
    "\n",
    "The `DST` column is one of the key attributes in our dataset. Here, we calculate descriptive statistics for this column to understand its distribution, including metrics like mean, standard deviation, and range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76232a07-ff58-4860-841b-e00897b05ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DST\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7882cc6-8253-4711-ab3c-2862643e81ac",
   "metadata": {},
   "source": [
    "## üö® Checking for Missing Values\n",
    "\n",
    "Finally, we check for missing values in the `DST` column. Missing data can affect our analysis, so it's important to identify any gaps early on so they can be handled appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5347683-31e6-484d-9a9f-652fa736fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ddc38-0e48-49c8-a1f5-1fea5d2bd559",
   "metadata": {},
   "source": [
    "## üîó Correlation Analysis\n",
    "\n",
    "In this section, we will perform a correlation analysis on our dataset to understand the relationships between different attributes. Correlation measures how strongly two variables are related to each other, with values ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation). \n",
    "\n",
    "### Steps:\n",
    "1. **Calculate the Correlation Matrix**: We start by calculating the correlation matrix for the entire dataset. This matrix will show us the correlation coefficients between all pairs of attributes.\n",
    "  \n",
    "2. **Determine the Total Number of Attributes**: Next, we determine the total number of attributes in our dataset. This will help us in dividing the attributes into two halves for easier visualization.\n",
    "\n",
    "3. **Splitting Attributes into Two Halves**: To make the heatmaps more readable, we split the attributes into two halves:\n",
    "   - **First Half**: Contains the first half of the attributes.\n",
    "   - **Second Half**: Contains the second half of the attributes.\n",
    "   \n",
    "4. **Heatmap for the First Half**: We create a heatmap for the first half of the attributes, showing how each attribute correlates with `DST`. The `vlag` color palette is used to intuitively represent the correlation, with the color scale ranging from -1 to +1.\n",
    "  \n",
    "5. **Heatmap for the Second Half**: Similarly, we create a heatmap for the second half of the attributes, focusing again on their correlation with `DST`.\n",
    "\n",
    "### Visualization Goals:\n",
    "- **Identify Strong Correlations**: The heatmaps will help us quickly identify which attributes have strong positive or negative correlations with `DST`.\n",
    "- **Focus on Important Attributes**: By visualizing the correlations, we can focus on the most significant attributes for further analysis or model development.\n",
    "\n",
    "The heatmaps will be saved as images (`corr1.png` and `corr2.png`) for easy reference and sharing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fbcd6-6ce6-4e6a-b521-2c5e7922e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that only numeric columns are selected for the correlation matrix\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation matrix only for numeric columns\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Determine the total number of attributes\n",
    "total_attributes = len(correlation_matrix.columns)\n",
    "\n",
    "# Find the middle index to split the attributes into two halves\n",
    "mid_index = total_attributes // 2\n",
    "\n",
    "# First half of the attributes\n",
    "first_half = correlation_matrix.columns[:mid_index]\n",
    "\n",
    "# Second half of the attributes\n",
    "second_half = correlation_matrix.columns[mid_index:]\n",
    "\n",
    "# Create a heatmap for the first half of attributes in correlation with DST\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(correlation_matrix.loc[['DST'], first_half], annot=True, fmt=\".2f\", cmap=\"vlag\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation heatmap for the first half of attributes with DST')\n",
    "plt.savefig(\"../results/corr1\")  # Save the figure as \"corr1\"\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap for the second half of attributes in correlation with DST\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.heatmap(correlation_matrix.loc[['DST'], second_half], annot=True, fmt=\".2f\", cmap=\"vlag\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation heatmap for the second half of attributes with DST')\n",
    "plt.savefig(\"../results/corr2\")  # Save the figure as \"corr2\"\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3734c89-fb7d-4d52-b4d7-7a1dc702c99b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Data Preparation and Feature Selection\n",
    "\n",
    "In this section, we proceed with data preparation and feature selection, focusing on identifying the most relevant attributes for predicting the `DST` variable:\n",
    "\n",
    "1. **Creating a New DataFrame for Analysis**: We create a new DataFrame `df_analysis` for the current experiment to ensure that the original data, including the `time1` attribute, is preserved for future use.\n",
    "\n",
    "2. **Removing the `time1` Attribute**: The `time1` column, which is not needed for this specific analysis, is removed from `df_analysis`. This allows us to focus on the other attributes without introducing potential noise from the timestamp data.\n",
    "\n",
    "3. **Handling Missing Values**: Any missing values in `df_analysis` are replaced with the median value of their respective columns. This step ensures that the dataset is complete and ready for further analysis.\n",
    "\n",
    "4. **Target Variable (`y`)**: The target variable `DST` is extracted from `df_analysis` and stored in the variable `y`. This will be the variable we aim to predict.\n",
    "\n",
    "5. **Input Attributes (`X`)**: All input attributes, excluding `DST`, are stored in the variable `X`. These attributes will be used as predictors in our model.\n",
    "\n",
    "6. **Feature Labels**: The names of the input attributes (excluding `DST`) are stored in the variable `feature_labels`. This will help us identify which features are selected as most relevant.\n",
    "\n",
    "7. **Feature Selection**: \n",
    "   - We use the `SelectKBest` model with the `f_regression` scoring function to select the top 10 most relevant features for predicting `DST`.\n",
    "   - The selected features are those that have the highest correlation with the target variable `DST`, as measured by the F-score.\n",
    "\n",
    "8. **Visualizing the Selected Features**: A horizontal bar chart is plotted to visualize the top 10 selected features based on their F-scores. This visualization helps to identify which attributes are most significant for predicting `DST`.\n",
    "\n",
    "9. **Output Selected Features**: Finally, the selected features and their corresponding F-scores are printed for further inspection. This output provides a clear understanding of which features are most important for the prediction task.\n",
    "\n",
    "The resulting plot is saved as \"KBest.png\" and displayed for easy reference and documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ca861-00a9-46f7-8a9c-d8e7f9b27792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for the current analysis\n",
    "df_analysis = df.copy()\n",
    "\n",
    "# Removing the time1 attribute from df_analysis for the current experiment\n",
    "df_analysis = df_analysis.drop(columns=['time1'])\n",
    "\n",
    "# Replacing missing values with the median in df_analysis\n",
    "df_analysis.fillna(df_analysis.median(), inplace=True)\n",
    "\n",
    "# The target variable DST is stored in the variable y\n",
    "y = df_analysis['DST'].values\n",
    "\n",
    "# Input attributes are stored in the variable X (all columns except DST)\n",
    "X = df_analysis.drop(columns=['DST']).values\n",
    "\n",
    "# The names of the attributes (except DST) are stored in the variable feature_labels\n",
    "feature_labels = df_analysis.drop(columns=['DST']).columns.values\n",
    "\n",
    "# The SelectKBest model with the f_regression score function is used to select the 10 best attributes (numFeatures).\n",
    "numFeatures = 10\n",
    "fSelect_model = SelectKBest(score_func=f_regression, k=numFeatures)\n",
    "X_fSelect = fSelect_model.fit_transform(X, y)\n",
    "scores = fSelect_model.scores_\n",
    "order = np.argsort(scores)\n",
    "ordered_feature_labels = feature_labels[order]\n",
    "y_pos = np.arange(len(feature_labels))\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(y_pos[-numFeatures:], scores[order][-numFeatures:], align='center', color=\"#EFBF7B\")\n",
    "plt.yticks(y_pos[-numFeatures:], ordered_feature_labels[-numFeatures:], fontsize=12)\n",
    "plt.xlabel('F-score', fontsize=15)\n",
    "plt.title('Top 10 Features Based on F-score', fontsize=15)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6) \n",
    "plt.savefig(\"../results/KBest\")\n",
    "plt.show()\n",
    "\n",
    "# Extracting selected feature labels and their scores\n",
    "selected_feature_labels = ordered_feature_labels[-numFeatures:]\n",
    "selected_scores = scores[order][-numFeatures:]\n",
    "\n",
    "# Printing selected features and their F-scores\n",
    "print(\"Selected features and their F-scores:\")\n",
    "for label, score in zip(selected_feature_labels, selected_scores):\n",
    "    print(f\"{label}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a09cc-ce65-41f0-a326-24a85b4cb02f",
   "metadata": {},
   "source": [
    "## üìä Data Splitting for Training, Validation, and Testing\n",
    "\n",
    "In this section, we split the dataset into training, validation, and test sets. This is a crucial step in machine learning workflows to ensure that the model is trained, validated, and tested on separate portions of the data. Here‚Äôs how the data is divided:\n",
    "\n",
    "1. **Convert `time1` to Datetime**: The `time1` column is converted to a datetime format to facilitate time-based splitting.\n",
    "\n",
    "2. **Splitting the Data**:\n",
    "   - The data is split into training and test sets based on a specific date (`2003-11-19`).\n",
    "   - The training set contains all data points before this date, while the test set contains all data points from this date onwards.\n",
    "\n",
    "3. **Creating the Validation Set**:\n",
    "   - The last 20% of the training set is further split off to create a validation set. This set is used to tune and validate the model during training.\n",
    "\n",
    "4. **Visualizing the Data Splits**:\n",
    "   - The `DST` index is plotted over time for the training, validation, and test sets. Different colors are used to distinguish between the different sets, making it easier to visualize how the data is divided.\n",
    "\n",
    "5. **Saving the Splits**:\n",
    "   - The resulting training and test datasets are saved to CSV files (`train_omni.csv` and `test_omni.csv`) for future use in model training and evaluation.\n",
    "\n",
    "This structured approach ensures that the model can be trained on one part of the data, validated on another, and finally tested on unseen data to evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b56c91-39d7-447c-9d4f-efa29c182073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the time1 column to datetime format\n",
    "df['time1'] = pd.to_datetime(df['time1'])\n",
    "\n",
    "# Split the data based on the date '2003-11-19'\n",
    "split_index = df[df['time1'] >= '2003-11-19'].index.min()\n",
    "train = df.iloc[:split_index, :]\n",
    "test = df.iloc[split_index:, :]\n",
    "\n",
    "# Display the last few rows of the training set to verify the split\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52ecf8-4877-400a-894c-76ed79bccdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the test set to verify the split\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5462261-34b8-4f0b-bc7d-c9d0b6535dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns in the test set\n",
    "test = test[[\"time1\", \"KP\", \"DST\"]]\n",
    "\n",
    "# Print the number of records in the test and train sets\n",
    "print(len(test))\n",
    "print(len(train))\n",
    "\n",
    "# Create a validation set from the last 20% of the training data\n",
    "valid_size = int(len(train) * 0.2)\n",
    "valid = train.iloc[-valid_size:, :].copy()\n",
    "train = train.iloc[:-valid_size, :].copy()\n",
    "\n",
    "# Plotting the DST index for training, validation, and test sets\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.figure()\n",
    "plt.title(\"DST Index - Data Splitting\")\n",
    "plt.plot(train['time1'], train['DST'], label='Training Data', color=\"#FFA07A\") \n",
    "plt.plot(valid['time1'], valid['DST'], label='Validation Data', color=\"#20B2AA\") \n",
    "plt.plot(test['time1'], test['DST'], label='Test Data', color=\"#9370DB\")  \n",
    "plt.savefig(\"../results/dataset_cut.png\")\n",
    "plt.legend()\n",
    "\n",
    "# Save the test and train datasets to CSV files\n",
    "test.to_csv('../data/test_omni.csv')\n",
    "train.to_csv('../data/train_omni.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355f618-7045-47e8-8691-268d7528ccfd",
   "metadata": {},
   "source": [
    "# üîÑ Autoregressive Model Data Preparation\n",
    "\n",
    "In this section, we prepare the data for training an autoregressive model. An autoregressive model predicts the future values of a time series based on its own previous values. T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b54e3e-c0c0-411f-b174-2a7f5c35de89",
   "metadata": {},
   "source": [
    "## ‚è≥ Time Series Data Preparation and Generators\n",
    "\n",
    "In this section, we prepare the time series data and set up the generators needed for model training, validation, and testing.\n",
    "\n",
    "### What is a TimeseriesGenerator?\n",
    "\n",
    "The `TimeseriesGenerator` is a powerful tool for preparing sequential data for time series forecasting models. It automatically generates batches of input-output pairs for your model by sliding a window over your dataset. The generator allows you to specify how many previous time steps (`n_input`) to include for each prediction.\n",
    "\n",
    "### How Does It Work?\n",
    "\n",
    "1. **Sliding Window**: \n",
    "   - The generator slides a window of a specified length (`n_input`) over the dataset. For each position of the window, it extracts a sequence of past observations as the input (`X`) and the next observation as the output (`y`).\n",
    "\n",
    "2. **Batch Creation**: \n",
    "   - These sequences are grouped into batches, with the size of each batch defined by `batch_size`. During training, the model processes one batch at a time.\n",
    "\n",
    "3. **Example**:\n",
    "   - Suppose you have a dataset of daily temperatures over 10 days: `[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`.\n",
    "   - With `n_input=3` and `batch_size=2`, the `TimeseriesGenerator` would create the following sequences:\n",
    "     - **Batch 1**:\n",
    "       - `X`: `[[10, 11, 12], [11, 12, 13]]`\n",
    "       - `y`: `[13, 14]`\n",
    "     - **Batch 2**:\n",
    "       - `X`: `[[12, 13, 14], [13, 14, 15]]`\n",
    "       - `y`: `[15, 16]`\n",
    "   - The generator continues sliding the window across the data to create all possible sequences.\n",
    "\n",
    "### Setting Up TimeseriesGenerator\n",
    "\n",
    "We define the following parameters:\n",
    "\n",
    "- **n_input**: The number of previous time steps to consider for each prediction.\n",
    "- **b_size**: The number of samples to process in each batch.\n",
    "- **n_features**: The number of features in the dataset (in this case, it's the length of the training data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d45b0-536b-4085-984e-af13d1de7738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Define the target column to predict\n",
    "y_col = 'DST'\n",
    "\n",
    "# Extract the target (y) and input (X) variables for training, validation, and test sets\n",
    "y_train = train[y_col].values.copy()\n",
    "X_train = train['DST'].values.copy()\n",
    "y_val = valid[y_col].values.copy()\n",
    "X_val = valid['DST'].values.copy()\n",
    "y_test = test[y_col].values.copy()\n",
    "X_test = test['DST'].values.copy()\n",
    "\n",
    "# Time series generator settings\n",
    "n_input = 6  # Number of previous time steps to consider for each prediction\n",
    "n_features = len(X_train) # Number of features in the input data (DST)\n",
    "b_size = 256  # Batch size\n",
    "\n",
    "# Create TimeseriesGenerators for training, validation, and test sets\n",
    "train_generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=b_size)\n",
    "val_generator = TimeseriesGenerator(X_val, y_val, length=n_input, batch_size=b_size)\n",
    "test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=256)\n",
    "\n",
    "# Inspect the generators\n",
    "print(\"Number of batches: \", len(train_generator))\n",
    "print(\"Each batch contains features (X component) and labels (y component): \", len(train_generator[0]))\n",
    "print(\"Length of the X component of a batch: \", len(train_generator[0][0]))\n",
    "print(\"Length of the y component of a batch (number of measurements in the batch): \", len(train_generator[0][1]))\n",
    "print(\"Number of rows considered for one measurement (how many steps back): \", len(train_generator[0][0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222339cc-057a-4ba8-973f-a302c3c6a48b",
   "metadata": {},
   "source": [
    "## üîç Inspecting the First Batch from TimeseriesGenerator\n",
    "\n",
    "In this section, we verify the correctness of the `TimeseriesGenerator` by inspecting the first batch generated from the training data. This process involves the following steps:\n",
    "\n",
    "1. **Extract the First Batch**:\n",
    "   - We extract the first batch of input sequences (`X_train_batch`) and target values (`y_train_batch`) from the training generator.\n",
    "\n",
    "2. **Print the Batch**:\n",
    "   - We print the contents of the first batch to visually inspect the sequences and their corresponding target values.\n",
    "\n",
    "3. **Manual Verification**:\n",
    "   - We manually verify the first few sequences to ensure that the `TimeseriesGenerator` is correctly sliding over the data and generating the expected input-output pairs.\n",
    "\n",
    "This inspection step helps ensure that the data preparation is functioning as intended before moving on to model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d4c2e-7b73-46e5-a7b5-6968c9152d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the first batch from the train generator to verify correctness\n",
    "X_train_batch, y_train_batch = train_generator[0]\n",
    "\n",
    "print(\"First batch from the training generator:\")\n",
    "print(\"X (input sequences):\")\n",
    "print(X_train_batch)\n",
    "print(\"y (target values):\")\n",
    "print(y_train_batch)\n",
    "print()\n",
    "\n",
    "# Manually verify the first few sequences\n",
    "print(\"Manually verifying the first few sequences:\")\n",
    "for i in range(min(5, len(X_train_batch))):  # Verify first 5 sequences or less if batch size is smaller\n",
    "    print(f\"Sequence {i+1}:\")\n",
    "    print(f\"X: {X_train_batch[i]}\")\n",
    "    print(f\"Expected y: {y_train_batch[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c66ca-2b20-46df-9492-93b107738eaa",
   "metadata": {},
   "source": [
    "## üöÄ Training an Autoregressive LSTM Model\n",
    "\n",
    "In this section, we build, compile, and train a Bidirectional LSTM model designed to predict future values of the `DST` index based on its past values. The model is trained using the sequences generated by the `TimeseriesGenerator` and incorporates techniques to ensure robust training and prevent overfitting.\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "1. **Bidirectional LSTM Layers**:\n",
    "   - The model begins with a `Bidirectional LSTM` layer, allowing the model to capture patterns in the time series data in both forward and backward directions.\n",
    "   - This is followed by another `LSTM` layer to further process the sequential data.\n",
    "\n",
    "2. **TimeDistributed Layer**:\n",
    "   - A `TimeDistributed` dense layer applies the same dense layer to each time step of the sequence, maintaining the temporal structure of the data.\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - The output from the sequence is flattened and passed through a dense layer to predict the next value in the time series.\n",
    "\n",
    "4. **Model Compilation**:\n",
    "   - The model is compiled with Mean Squared Error (`mse`) as the loss function and the Adam optimizer for efficient training. The Mean Absolute Error (`mae`) is used as a metric to monitor performance.\n",
    "\n",
    "### Model Training:\n",
    "\n",
    "1. **Checkpoint and Early Stopping**:\n",
    "   - The model is trained with callbacks for saving the best model (based on validation MAE) and stopping early if the validation performance does not improve, which helps prevent overfitting.\n",
    "\n",
    "2. **Training Process**:\n",
    "   - The model is trained over 200 epochs, with both training and validation data fed from the `TimeseriesGenerator`.\n",
    "\n",
    "### Prediction:\n",
    "\n",
    "- After training, the best model can be loaded, and predictions can be made on the test data using the trained LSTM model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c2aa1-665f-4514-8add-08b4fdd1ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based autoregressive model\n",
    "inputs = Input(shape=(n_input, 1))\n",
    "layers = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(inputs)\n",
    "layers = LSTM(128, return_sequences=True)(layers)\n",
    "layers = TimeDistributed(Dense(1, activation='linear'))(layers)\n",
    "output = Flatten()(layers)\n",
    "output = Dense(1, activation='linear')(output)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=[\"mae\"])\n",
    "print(model.summary())\n",
    "\n",
    "# Define the model checkpoint and early stopping callbacks\n",
    "saved_model = \"../models/1h_dopredu_DST.keras\"\n",
    "checkpoint = ModelCheckpoint(saved_model, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_mae\", mode=\"min\", patience=25)\n",
    "callbacks_list = [checkpoint, early]\n",
    "\n",
    "# Train the model with the TimeseriesGenerator\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=100, verbose=1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e8fbc-5e3a-474e-8ed1-ff33ae336cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "model = load_model(saved_model)\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e85bb4-d053-41f7-ba62-2817463f762b",
   "metadata": {},
   "source": [
    "## üìä Signal Comparison Metrics\n",
    "\n",
    "In this section, we evaluate the performance of the model in predicting the `DST` values using various signal comparison metrics. These metrics help us understand the magnitude of the error, the correlation between predicted and actual values, and the overall signal quality.\n",
    "\n",
    "### 1. Mean Squared Error (MSE)\n",
    "- **Definition**: MSE measures the average of the squares of the errors‚Äîthat is, the average squared difference between the predicted and actual values. It provides a measure of the magnitude of the error.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  \n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "- **Definition**: RMSE is the square root of MSE and provides an error metric in the same units as the predicted values, making it easier to interpret.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "\n",
    "### 3. Pearson Correlation Coefficient (PCC)\n",
    "- **Definition**: PCC measures the linear correlation between the predicted and actual values. It ranges from `-1` to `1`, with `1` indicating a perfect positive correlation.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  \\text{PCC} = \\frac{\\text{Cov}(y, \\hat{y})}{\\sigma_y \\sigma_{\\hat{y}}}\n",
    "  $$\n",
    "\n",
    "### 4. Signal-to-Noise Ratio (SNR)\n",
    "- **Definition**: SNR compares the level of the desired signal to the level of background noise. A higher SNR indicates that the signal is much stronger than the noise.\n",
    "- **Formula**: \n",
    "  $$\n",
    "  \\text{SNR} = 10 \\log_{10}\\left(\\frac{\\sum_{i=1}^{n} y_i^2}{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\right)\n",
    "  $$\n",
    "  \n",
    "- **Interpretation**:\n",
    "  - **High SNR**: Indicates a strong signal compared to noise. Generally, an SNR above 20 dB is considered good, with values above 40 dB being excellent.\n",
    "  - **Low SNR**: Indicates that the noise level is comparable to or greater than the signal level, which can make the signal difficult to distinguish. An SNR below 10 dB is typically considered poor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad85f4-9ccf-477c-aafd-da9142bc184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Signal Comparison Metrics\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test[n_input:], y_pred.reshape(-1))\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Calculate Pearson Correlation Coefficient (PCC)\n",
    "pcc, _ = pearsonr(y_test[n_input:], y_pred.reshape(-1))\n",
    "print(f\"Pearson Correlation Coefficient: {pcc}\")\n",
    "\n",
    "# Calculate Signal-to-Noise Ratio (SNR)\n",
    "snr = 10 * np.log10(np.sum(y_test[n_input:]**2) / np.sum((y_test[n_input:] - y_pred.reshape(-1))**2))\n",
    "print(f\"Signal-to-Noise Ratio: {snr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c74865-ce6d-4450-8d63-a50154f7fa09",
   "metadata": {},
   "source": [
    "## üßÆ Classification Metrics: Evaluating Model Performance with MCC and Other Metrics\n",
    "\n",
    "In this section, we evaluate the performance of the trained LSTM model on the validation set using various classification metrics, including the Matthews Correlation Coefficient (MCC), confusion matrix, classification report, and accuracy. These metrics provide a comprehensive understanding of the model's ability to distinguish between classes, particularly in imbalanced datasets.\n",
    "\n",
    "### 1. Threshold Selection Using MCC\n",
    "\n",
    "The first step in evaluating the model's performance is to determine the optimal threshold for classifying `DST` values as events or non-events. This is done using the Matthews Correlation Coefficient (MCC).\n",
    "\n",
    "#### What is MCC?\n",
    "\n",
    "The Matthews Correlation Coefficient (MCC) is a single-value measure that takes into account true and false positives and negatives, providing a balanced evaluation of classification performance. It returns a value between `-1` and `+1`:\n",
    "- **`+1`**: Indicates a perfect prediction.\n",
    "- **`0`**: Indicates no better performance than random guessing.\n",
    "- **`-1`**: Indicates total disagreement between predictions and actual values.\n",
    "\n",
    "#### Advantages of Using MCC:\n",
    "\n",
    "1. **Handles Imbalanced Datasets**:\n",
    "   - Unlike accuracy, MCC provides a more informative and reliable score when the classes are imbalanced. This makes it ideal for datasets where one class (e.g., extreme `DST` events) is much rarer than the other.\n",
    "\n",
    "2. **Balanced Evaluation**:\n",
    "   - MCC considers all four quadrants of the confusion matrix (true positives, false positives, true negatives, and false negatives), offering a balanced evaluation even when one class is underrepresented.\n",
    "\n",
    "3. **Single Metric**:\n",
    "   - MCC combines the information from precision, recall, and specificity into a single metric, making it easier to compare the performance of different models or thresholds without needing to calculate multiple metrics.\n",
    "\n",
    "#### Steps for Threshold Selection:\n",
    "\n",
    "1. **Prediction on Validation Set**:\n",
    "   - We use the trained model to predict the `DST` values on the validation set using the `TimeseriesGenerator`.\n",
    "\n",
    "2. **Binary Classification of `DST` Values**:\n",
    "   - The true labels are determined by setting a threshold of `-20` on the `DST` index. Values less than or equal to `-20` are classified as `1` (event), and others as `0` (non-event).\n",
    "\n",
    "3. **Threshold Tuning**:\n",
    "   - We calculate the MCC for a range of thresholds (`-10` to `-40` with a step of `-0.1`) to determine the threshold that yields the best MCC score.\n",
    "   - The best threshold is identified as the one that maximizes the MCC.\n",
    "\n",
    "4. **Results**:\n",
    "   - The best threshold and its corresponding MCC are printed, providing insights into the optimal cutoff for event detection based on the model‚Äôs predictions.\n",
    "\n",
    "### 2. Confusion Matrix, Classification Report, and Accuracy\n",
    "\n",
    "Once the optimal threshold is selected using MCC, we evaluate the model's classification performance using additional metrics.\n",
    "\n",
    "#### Confusion Matrix:\n",
    "- **Definition**: The confusion matrix provides a detailed breakdown of the classification outcomes, showing the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "#### Classification Report:\n",
    "- **Definition**: The classification report provides key metrics like precision, recall, and F1-score for each class, giving a more detailed insight into the model's performance across different classes.\n",
    "\n",
    "#### Accuracy (ACC):\n",
    "- **Definition**: Accuracy is the ratio of correctly predicted instances (both true positives and true negatives) to the total instances. It provides a general measure of how often the model is correct.\n",
    "- **Formula**: $$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e172fe8-73f1-4046-b9b7-49d16c9b2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_val_pred = model.predict(val_generator)\n",
    "\n",
    "# Create binary true labels for the validation set based on a threshold of -20\n",
    "true_labels_val = np.where(y_val[n_input:] <= -20, 1, 0)\n",
    "\n",
    "# Function to calculate the Matthews Correlation Coefficient (MCC)\n",
    "def calculate_mcc(y_true, y_pred):\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Explore different thresholds to find the best MCC\n",
    "thresholds = np.arange(-10, -40, -0.1)\n",
    "mcc_scores = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # Convert predictions to binary labels based on the current threshold\n",
    "    predictions = np.where(y_val_pred <= thresh, 1, 0)\n",
    "    # Calculate MCC for the current threshold\n",
    "    mcc = calculate_mcc(true_labels_val, predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "# Find the threshold with the highest MCC\n",
    "best_threshold_mcc = thresholds[np.argmax(mcc_scores)]\n",
    "best_mcc = max(mcc_scores)\n",
    "\n",
    "print(\"Best Threshold for MCC on Validation Set:\", best_threshold_mcc)\n",
    "print(\"Best MCC on Validation Set:\", best_mcc)\n",
    "\n",
    "# Optional: Round the best threshold for reporting\n",
    "best_threshold_mcc_rounded = round(best_threshold_mcc, 1)\n",
    "print(\"Best Threshold for MCC (Rounded):\", best_threshold_mcc_rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbce3b-ab77-4c24-9fe6-68aca8f243a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to binary classifications based on the best threshold\n",
    "y_pred_best_th = np.where(y_test[n_input:] <= best_threshold_mcc_rounded, 1, 0)\n",
    "true_labels_test = np.where(y_test[n_input:] <= -20, 1, 0)\n",
    "\n",
    "# Calculate Accuracy (ACC)\n",
    "acc = accuracy_score(true_labels_test, y_pred_best_th)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "# Calculate MCC on the test set\n",
    "mcc_test = matthews_corrcoef(true_labels_test, y_pred_best_th)\n",
    "print(f\"Test Set MCC: {mcc_test}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels_test, y_pred_best_th)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(true_labels_test, y_pred_best_th))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c0ea9-4bd4-48a3-bf20-e1814efcf009",
   "metadata": {},
   "source": [
    "## üìà Visualizing Model Predictions vs. True Values\n",
    "\n",
    "In this section, we create visualizations to compare the predicted values of the DST index with the true values. We use different colors for the true and predicted values to make the comparison clear and visually appealing.\n",
    "\n",
    "## Full Data Visualization\n",
    "\n",
    "First, we plot the true and predicted DST index values over the entire time period. This helps us get an overall sense of how well the model's predictions align with the actual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd278dc-998b-400c-a6c9-27e20c0ac958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with time, true values, and predicted values\n",
    "df = pd.DataFrame(data={\n",
    "    \"time\": test['time1'][n_input:], \n",
    "    \"y_true\": y_test[n_input:].reshape(-1), \n",
    "    \"y_predict\": y_pred.reshape(-1)\n",
    "})\n",
    "\n",
    "# Plotting the full data with enhanced colors\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Predicting the Change in DST Index', fontsize=16)\n",
    "plt.plot(df['time'], df['y_true'], label='True Values', color='#17becf', linewidth=2)  # Green color for true values\n",
    "plt.plot(df['time'], df['y_predict'], label='Predicted Values', color='#9467bd', linewidth=2)  # Red color for predicted values\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('DST Index', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316979b-a8a3-4690-82b8-0563e6c85ca3",
   "metadata": {},
   "source": [
    "## Zoomed-In Data Visualization\n",
    "\n",
    "Next, we provide a zoomed-in view of the DST index over a smaller time interval. This detailed view allows us to closely examine the accuracy of the model's predictions in a specific segment of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874fc16-38c1-431e-9f48-7f0304aeb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zoomed-in view for a smaller time interval\n",
    "# Define the interval (e.g., the first 200 time steps for detailed view)\n",
    "zoom_interval = 200\n",
    "df_zoomed = df.iloc[:zoom_interval]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title('Zoomed-In: Predicting the Change in DST Index', fontsize=16)\n",
    "plt.plot(df_zoomed['time'], df_zoomed['y_true'], label='True Values', color='#17becf', linewidth=2) \n",
    "plt.plot(df_zoomed['time'], df_zoomed['y_predict'], label='Predicted Values', color='#9467bd', linewidth=2) \n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.ylabel('DST Index', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a7dde-6932-46bd-b1d8-2b59422aef7b",
   "metadata": {},
   "source": [
    "## üìÖ Interactive Date-Based Visualization (Limited to Test Set Range)\n",
    "\n",
    "In this section, you can interactively select a date range within the limits of the test set to visualize the predicted versus true values of the DST index. Use the date pickers below to choose the start and end dates. The plot will automatically update to reflect the selected time interval.\n",
    "\n",
    "The date pickers are constrained to the date range of the test set, ensuring that you only select valid dates for which predictions are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253d3b1-acfe-4298-9fcd-a203531f3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Get the minimum and maximum dates from the test set\n",
    "min_date = test['time1'].min().date()\n",
    "max_date = test['time1'].max().date()\n",
    "\n",
    "# Create date picker widgets for selecting the start and end dates\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description='Start Date',\n",
    "    value=min_date,  # Set the default start date to the minimum date in the test set\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description='End Date',\n",
    "    value=max_date,  # Set the default end date to the maximum date in the test set\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the date pickers\n",
    "display(start_date_picker, end_date_picker)\n",
    "\n",
    "def update_plot_by_date(start_date, end_date):\n",
    "    # Ensure the dates are within the test set range\n",
    "    if start_date is not None and end_date is not None:\n",
    "        if start_date < min_date:\n",
    "            start_date = min_date\n",
    "        if end_date > max_date:\n",
    "            end_date = max_date\n",
    "            \n",
    "        # Filter the DataFrame based on the selected date range\n",
    "        mask = (df['time'] >= pd.Timestamp(start_date)) & (df['time'] <= pd.Timestamp(end_date))\n",
    "        df_filtered = df.loc[mask]\n",
    "        \n",
    "        # Plotting the data within the selected date range\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.title(f'Predicting the Change in DST Index from {start_date} to {end_date}', fontsize=16)\n",
    "        plt.plot(df_filtered['time'], df_filtered['y_true'], label='True Values', color='#17becf', linewidth=2)  # Cyan color for true values\n",
    "        plt.plot(df_filtered['time'], df_filtered['y_predict'], label='Predicted Values', color='#9467bd', linewidth=2)  # Purple color for predicted values\n",
    "        plt.legend(loc='upper right', fontsize=12)\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.xlabel('Time', fontsize=14)\n",
    "        plt.ylabel('DST Index', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# Link the function to the date pickers using an interactive widget\n",
    "widgets.interactive(update_plot_by_date, start_date=start_date_picker, end_date=end_date_picker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda97c37-9b12-4177-9d21-8bb71963ec0d",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Next Steps: Enhancing Your Time Series Model\n",
    "\n",
    "Now that you've successfully built and evaluated a model for predicting the DST index, it's time to explore some extensions and enhancements. These next steps are designed to help you deepen your understanding of time series forecasting and neural network modeling.\n",
    "\n",
    "### 1. Predicting Further Into the Future\n",
    "\n",
    "Currently, the model is set up to predict the DST index one hour ahead with a window size of 6 time steps. However, you can modify the model to predict further into the future (e.g., 3 hours, 6 hours) or adjust the window size to see how it affects the predictions.\n",
    "\n",
    "**How to Adjust the Prediction Window:**\n",
    "- Change the prediction window to forecast multiple hours ahead by adjusting the number of past time steps (window size) the model uses to make predictions.\n",
    "- For example, you might increase the window size from 6 to 18 to predict 3 hours ahead.\n",
    "- Modify the neural network architecture accordingly to handle the increased complexity when predicting further into the future.\n",
    "\n",
    "### 2. Adding More Features to the Neural Network\n",
    "\n",
    "The DST index is influenced by various factors, including the KP index, solar wind speed, and other space weather parameters. By adding these features to your neural network, you may improve its predictive power.\n",
    "\n",
    "**Steps to Add More Features:**\n",
    "- Include additional features in your dataset, such as the KP index, to provide the model with more information.\n",
    "- Adjust the input shape of your neural network to account for the additional features.\n",
    "\n",
    "### 3. Experimenting with Neural Network Architecture\n",
    "\n",
    "Explore different neural network architectures to see how they affect model performance. Consider trying different types of layers, activation functions, or even completely different models like GRU (Gated Recurrent Unit) or Transformer-based models.\n",
    "\n",
    "**Ideas for Architecture Modifications:**\n",
    "- Increase the number of LSTM units to capture more complex patterns in the data.\n",
    "- Add dropout layers to help prevent overfitting by randomly setting a fraction of input units to 0 during training.\n",
    "- Experiment with other recurrent layers like GRU instead of LSTM, or even a combination of both.\n",
    "\n",
    "### 4. Exploring Other Time Series Models\n",
    "\n",
    "Beyond LSTMs, there are various other models you can experiment with:\n",
    "- **ARIMA (AutoRegressive Integrated Moving Average)**: A popular statistical method for time series forecasting.\n",
    "- **Prophet**: A model developed by Facebook for forecasting time series data, particularly effective with missing data and outliers.\n",
    "- **Transformer Models**: Explore the use of Transformer models for time series forecasting, which are particularly good at handling long-range dependencies.\n",
    "\n",
    "### 5. Evaluating with Different Metrics\n",
    "\n",
    "Expand your evaluation by using additional metrics like:\n",
    "- **R-squared (Coefficient of Determination)**: Measures the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "- **MAE (Mean Absolute Error)**: Provides a measure of errors between paired observations expressing the same phenomenon.\n",
    "\n",
    "### Summary\n",
    "\n",
    "These next steps provide you with a pathway to deepen your understanding and enhance your model's capabilities. By experimenting with different configurations, adjusting the window size, adding more features, and exploring new models, you'll gain valuable insights into the challenges and intricacies of time series forecasting in real-world scenarios.\n",
    "\n",
    "Feel free to experiment, iterate, and share your findings!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eddfa4-5bde-41ad-93c4-6329289aa593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
