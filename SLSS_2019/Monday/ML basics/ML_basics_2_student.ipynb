{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"ML_basics_2_student.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Qo3HJwwcS1du","colab_type":"text"},"source":["## Datasets"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BTjBVLnWS1dv","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/summer_school/ML_basics_student.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"szxmsXEzZ4Y2","colab_type":"code","colab":{}},"source":["!unzip ML_basics_student.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbkJ-d3CZ8oR","colab_type":"code","colab":{}},"source":["!mv ML_basics_student/* ."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yn0InjmwTXoT","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_classification, make_blobs\n","from matplotlib.colors import ListedColormap\n","from sklearn.datasets import load_breast_cancer\n","\n","cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n","\n","\n","# 1) our good old fruit dataset\n","fruits = pd.read_table('fruit_data_with_colors.txt')\n","\n","feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n","X_fruits = fruits[feature_names_fruits]\n","y_fruits = fruits['fruit_label']\n","target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n","\n","X_fruits_2d = fruits[['height', 'width']]\n","y_fruits_2d = fruits['fruit_label']\n","\n","\n","# 2) synthetic dataset for classification (binary) \n","plt.figure()\n","plt.title('Sample binary classification problem with two informative features')\n","X_C2, y_C2 = make_classification(n_samples=100, n_features=2,\n","                                n_redundant=0, n_informative=2,\n","                                n_clusters_per_class=1, flip_y=0.1,\n","                                class_sep=0.5, random_state=0)\n","plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n","           marker= 'o', s=50, cmap=cmap_bold)\n","plt.show()\n","\n","\n","# 3) more difficult synthetic dataset for classification (binary) \n","# with classes that are not linearly separable\n","X_D2, y_D2 = make_blobs(n_samples=100, n_features=2, centers=8,\n","                       cluster_std=1.3, random_state=4)\n","y_D2 = y_D2 % 2\n","plt.figure()\n","plt.title('Sample binary classification problem with non-linearly separable classes')\n","plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n","           marker= 'o', s=50, cmap=cmap_bold)\n","plt.show()\n","\n","\n","# 4) Real-world breast cancer dataset for classification\n","cancer = load_breast_cancer()\n","(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2r89X84rS1dy","colab_type":"text"},"source":["## Linear models for classification"]},{"cell_type":"markdown","metadata":{"id":"4lzQvD0PS1dy","colab_type":"text"},"source":["### Logistic regression"]},{"cell_type":"markdown","metadata":{"id":"vmYMdNR-S1dz","colab_type":"text"},"source":["#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)"]},{"cell_type":"code","metadata":{"id":"wqvtDXW2S1dz","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n","\n","fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n","y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n","X_train, X_test, y_train, y_test = (\n","train_test_split(X_fruits_2d.values,\n","                y_fruits_apple.values,\n","                random_state = 0))\n","\n","clf = LogisticRegression(C=100).fit(X_train, y_train)\n","plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n","                                         None, 'Logistic regression \\\n","for binary classification\\nFruit dataset: Apple vs others',\n","                                         subaxes)\n","\n","h = 6\n","w = 8\n","print('A fruit with height {} and width {} is predicted to be: {}'\n","     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n","\n","h = 10\n","w = 7\n","print('A fruit with height {} and width {} is predicted to be: {}'\n","     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n","subaxes.set_xlabel('height')\n","subaxes.set_ylabel('width')\n","\n","print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n","     .format(clf.score(X_train, y_train)))\n","print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n","     .format(clf.score(X_test, y_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgBDDnscS1d1","colab_type":"text"},"source":["#### Logistic regression on simple synthetic dataset"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"5UYBHT0oS1d2","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n","                                                   random_state = 0)\n","\n","fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n","C_reg = 1\n","clf = LogisticRegression(C=C_reg).fit(X_train, y_train)\n","title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(C_reg)\n","plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n","                                         None, None, title, subaxes)\n","\n","print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n","     .format(clf.score(X_train, y_train)))\n","print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n","     .format(clf.score(X_test, y_test)))\n","     "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAxPYrk_S1d4","colab_type":"text"},"source":["#### Breast cancer dataset"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"OCLkeXE6S1d5","colab_type":"code","colab":{}},"source":["cancer['DESCR']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SOaW3b77S1d7","colab_type":"code","colab":{}},"source":["cancer_df = pd.DataFrame(data=cancer['data'], columns=cancer['feature_names'])\n","cancer_df['target'] = cancer['target']\n","cancer_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5sd9F8vS1d9","colab_type":"text"},"source":["### Task 1: Which feature has the greatest variance? (after MinMax scaling)"]},{"cell_type":"code","metadata":{"id":"v-B2LR38S1d-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fZLZCcMS1eA","colab_type":"text"},"source":["### Task 2: Apply Logistic Regression to cancer dataset, include tuning the C parameter."]},{"cell_type":"code","metadata":{"id":"E1ZdfoT-S1eB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M699d7acS1eD","colab_type":"text"},"source":["### Support Vector Machines"]},{"cell_type":"markdown","metadata":{"id":"V3tcDRYcS1eE","colab_type":"text"},"source":["#### Linear Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"DwF9E7QtS1eF","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n","\n","fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n","C_reg = 1\n","clf = SVC(kernel='linear', C=C_reg).fit(X_train, y_train)\n","title = 'Linear SVC, C = {:.3f}'.format(C_reg)\n","plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQOu6Wx9S1eH","colab_type":"text"},"source":["### Task 3: Apply Linear SVM to cancer dataset, include tuning the C parameter."]},{"cell_type":"code","metadata":{"id":"08WyXtlQS1eI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xyCXnyVoS1eK","colab_type":"text"},"source":["### Kernelized Support Vector Machines"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"YAG3zyrgS1eK","colab_type":"code","colab":{}},"source":["from adspy_shared_utilities import plot_class_regions_for_classifier\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n","\n","# The default SVC kernel is radial basis function (RBF)\n","plot_class_regions_for_classifier(SVC().fit(X_train, y_train),\n","                                 X_train, y_train, None, None,\n","                                 'Support Vector Classifier: RBF kernel')\n","\n","# Compare decision boundries with polynomial kernel, degree = 3\n","plot_class_regions_for_classifier(SVC(kernel = 'poly', degree = 3)\n","                                 .fit(X_train, y_train), X_train,\n","                                 y_train, None, None,\n","                                 'Support Vector Classifier: Polynomial kernel, degree = 3')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D1IdmLXvS1eM","colab_type":"text"},"source":["#### Support Vector Machine with RBF kernel: gamma parameter"]},{"cell_type":"code","metadata":{"id":"AcOa9qfMS1eN","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n","fig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n","\n","for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n","    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n","    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n","    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n","                                             None, None, title, subplot)\n","    plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rrw7p3X6S1eP","colab_type":"text"},"source":["#### Application of SVMs to a real dataset: unnormalized data"]},{"cell_type":"code","metadata":{"id":"_YibipNiS1eQ","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n","                                                    random_state = 0)\n","\n","clf = SVC(C=10).fit(X_train, y_train)\n","print('Breast cancer dataset (unnormalized features)')\n","print('Accuracy of RBF-kernel SVC on training set: {:.2f}'\n","     .format(clf.score(X_train, y_train)))\n","print('Accuracy of RBF-kernel SVC on test set: {:.2f}'\n","     .format(clf.score(X_test, y_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3mlvLhHS1eR","colab_type":"text"},"source":["#### Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling"]},{"cell_type":"code","metadata":{"id":"sPEE2S9_S1eS","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","clf = SVC(C=100).fit(X_train_scaled, y_train)\n","print('Breast cancer dataset (normalized with MinMax scaling)')\n","print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.3f}'\n","     .format(clf.score(X_train_scaled, y_train)))\n","print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.3f}'\n","     .format(clf.score(X_test_scaled, y_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"B4wmb-CuS1eU","colab_type":"text"},"source":["## Decision Trees"]},{"cell_type":"code","metadata":{"id":"HZzf2gdlS1eV","colab_type":"code","colab":{}},"source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from adspy_shared_utilities import plot_decision_tree\n","from sklearn.model_selection import train_test_split\n","\n","iris = load_iris()\n","iris['feature_names']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PL3NQ7NAS1eX","colab_type":"text"},"source":["### Task 4: Plot histograms of all 4 features. Do you see something interesting?"]},{"cell_type":"code","metadata":{"id":"BymX2cN4S1eX","colab_type":"code","colab":{}},"source":["\"\"\" Use:\n","plt.figure()\n","plt.hist(...)\n","plt.show()\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBW2Zuh9S1ea","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\n","clf = DecisionTreeClassifier().fit(X_train, y_train)\n","\n","print('Accuracy of Decision Tree classifier on training set: {:.3f}'\n","     .format(clf.score(X_train, y_train)))\n","print('Accuracy of Decision Tree classifier on test set: {:.3f}'\n","     .format(clf.score(X_test, y_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Vgn1ExeS1ef","colab_type":"text"},"source":["#### Setting max decision tree depth to help avoid overfitting"]},{"cell_type":"code","metadata":{"id":"U0LUJBnvS1eg","colab_type":"code","colab":{}},"source":["clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n","\n","print('Accuracy of Decision Tree classifier on training set: {:.3f}'\n","     .format(clf2.score(X_train, y_train)))\n","print('Accuracy of Decision Tree classifier on test set: {:.3f}'\n","     .format(clf2.score(X_test, y_test)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ny7YVAw-S1ej","colab_type":"text"},"source":["#### Visualizing decision trees"]},{"cell_type":"code","metadata":{"id":"LAUllpyVS1ek","colab_type":"code","colab":{}},"source":["plot_decision_tree(clf, iris.feature_names, iris.target_names)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xayvl9r1S1em","colab_type":"text"},"source":["#### Pre-pruned version (max_depth = 3)"]},{"cell_type":"code","metadata":{"id":"9-GW0ryRS1em","colab_type":"code","colab":{}},"source":["plot_decision_tree(clf2, iris.feature_names, iris.target_names)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uha7jCbFS1eo","colab_type":"text"},"source":["#### Feature importance"]},{"cell_type":"code","metadata":{"id":"uP_FbZnUS1ep","colab_type":"code","colab":{}},"source":["from adspy_shared_utilities import plot_feature_importances\n","\n","plt.figure(figsize=(10,4), dpi=80)\n","plot_feature_importances(clf, iris.feature_names)\n","plt.show()\n","\n","print('Feature importances: {}'.format(clf.feature_importances_))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6kZBgxjS1er","colab_type":"text"},"source":["#### Decision Trees on a real-world dataset"]},{"cell_type":"markdown","metadata":{"id":"Uw_lexwsS1er","colab_type":"text"},"source":["### Task 5: Fit decision tree to the cancer dataset, tune the max_depth parameter, visualize the tree and plot feature importances."]}]}